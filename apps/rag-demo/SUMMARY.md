# 项目总结与反思 (Project Summary & Reflections)

**撰写人**: 产品研发专家  
**日期**: 2026-01-05

---

## 1. 核心价值回顾

本项目成功构建了一个基于 Mastra 框架的最小化 RAG 系统。回顾过去 30 年，我们从“人找信息”（雅虎目录）走到“搜索找信息”（Google），现在正迈入“AI 消化信息”（RAG + LLM）的时代。

这个 Demo 虽然简单，但它捕捉了这一时代的核心特征：
*   **非结构化数据的结构化**: 通过 Vector Embedding 将文本转化为机器可理解的数学向量。
*   **知识的动态注入**: 让 LLM 不再仅仅依赖预训练的记忆，而是能够实时获取最新的私有知识。

## 2. 遇到的挑战与解决方案 (Errors & Fixes)

在开发过程中，我们遇到并解决了一些关键问题，这些经验对于后续的大规模落地至关重要：

### 2.1 依赖注入与循环引用
*   **问题**: `Mastra` 实例需要 `Agent`，而 `Agent` 使用的 Tool 又依赖 `VectorStore`，`VectorStore` 又是 `Mastra` 的一部分。这在代码组织上容易产生循环引用。
*   **解决**: 采用了模块化设计。
    *   `tools.ts`: 独立定义 Tool，仅通过字符串名称 (`"pgVector"`) 引用 Store，而非直接引用实例。
    *   `mastra.ts`: 作为顶层容器，将 `agents` 和 `vectors` 组装在一起，负责依赖的最终解析。
    *   这种解耦设计体现了现代框架（如 NestJS, Angular）中常见的依赖注入思想。

### 2.2 向量库的选型与部署
*   **问题**: 为了保证“可运行”，我们不能假设用户拥有复杂的云端向量库环境。
*   **解决**: 选择了 `PGVector` + Docker 的方案。这既利用了 PostgreSQL 的成熟生态（ACID 事务、备份），又通过 Docker 实现了环境的一键交付。这是“老派”稳健工程与“新派”AI 技术的完美结合。

### 2.3 上下文窗口与 Token 成本
*   **思考**: 虽然现在的 LLM 支持 128k 甚至更长的 Context，但盲目丢入全文既昂贵又会引入噪音。
*   **决策**: 在 Ingestion 阶段采用了 512 token 的分块策略 (Chunking)。这是一个在精度（粒度够细）和连贯性（语义完整）之间的经验平衡点。

## 3. 未来展望 (Next Steps)

如果这是一个真实的商业项目，接下来的 Roadmap 应包含：
1.  **混合检索 (Hybrid Search)**: 结合关键词检索 (BM25) 和向量检索，解决专有名词匹配不准的问题。
2.  **重排序 (Reranking)**: 在检索结果进入 LLM 之前，引入 Cross-Encoder 模型进行二次排序，提高准确率。
3.  **多模态支持**: 不仅支持文本，还要支持 PDF 表格、图片的理解与索引。

---

> “技术在变，但用户对‘准确’与‘高效’的追求从未改变。”
